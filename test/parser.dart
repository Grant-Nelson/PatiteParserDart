part of PatiteParserDart.test;

void parser00(TestArgs args) {
  args.log('parser00');

  Tokenizer.Tokenizer tok = new Tokenizer.Tokenizer();
  tok.start("start");
  tok.join("start", "(")..addSet("(");
  tok.join("start", ")")..addSet(")");
  tok.join("start", "+")..addSet("+");
  tok.join("start", "num")..addRange("0", "9");
  tok.join("num", "num")..addRange("0", "9");
  tok.setToken("(", "(");
  tok.setToken(")", ")");
  tok.setToken("+", "+");
  tok.setToken("num", "n");
  // 1. E â†’ T
  // 2. E â†’ ( E )
  // 3. T â†’ n
  // 4. T â†’ + T
  // 5. T â†’ T + n
  Grammar.Grammar grammar = new Grammar.Grammar();
  grammar.start("E");
  grammar.newRule("E").addTerm("T");
  grammar.newRule("E").addToken("(").addTerm("E").addToken(")");
  grammar.newRule("T").addToken("n");
  grammar.newRule("T").addToken("+").addTerm("T");
  grammar.newRule("T").addTerm("T").addToken("+").addToken("n");
  Parser.Parser parser = new Parser.Parser.fromGrammar(grammar, tok);
  args.checkParser(parser, ["103"],
    ['â”€<E>',
     '  â””â”€<T>',
     '     â””â”€[n:3:"103"]']);
  args.checkParser(parser, ["+2"],
    ['â”€<E>',
     '  â””â”€<T>',
     '     â”œâ”€[+:1:"+"]',
     '     â””â”€<T>',
     '        â””â”€[n:2:"2"]']);
  args.checkParser(parser, ["3+4"],
    ['â”€<E>',
     '  â””â”€<T>',
     '     â”œâ”€<T>',
     '     â”‚  â””â”€[n:1:"3"]',
     '     â”œâ”€[+:2:"+"]',
     '     â””â”€[n:3:"4"]']);
  args.checkParser(parser, ["((42+6))"],
    ['â”€<E>',
     '  â”œâ”€[(:1:"("]',
     '  â”œâ”€<E>',
     '  â”‚  â”œâ”€[(:2:"("]',
     '  â”‚  â”œâ”€<E>',
     '  â”‚  â”‚  â””â”€<T>',
     '  â”‚  â”‚     â”œâ”€<T>',
     '  â”‚  â”‚     â”‚  â””â”€[n:4:"42"]',
     '  â”‚  â”‚     â”œâ”€[+:5:"+"]',
     '  â”‚  â”‚     â””â”€[n:6:"6"]',
     '  â”‚  â””â”€[):7:")"]',
     '  â””â”€[):8:")"]']);
}

void parser01(TestArgs args) {
  args.log('parser01');

  Tokenizer.Tokenizer tok = new Tokenizer.Tokenizer();
  tok.start("start");
  tok.join("start", "(")..addSet("(");
  tok.join("start", ")")..addSet(")");
  tok.setToken("(", "(");
  tok.setToken(")", ")");
  // 1. X â†’ ( X )
  // 2. X â†’ ( )
  Grammar.Grammar grammar = new Grammar.Grammar();
  grammar.start("X");
  grammar.newRule("X").addToken("(").addTerm("X").addToken(")");
  grammar.newRule("X").addToken("(").addToken(")");
  Parser.Parser parser = new Parser.Parser.fromGrammar(grammar, tok);
  args.checkParser(parser, ["()"],
    ['â”€<X>',
     '  â”œâ”€[(:1:"("]',
     '  â””â”€[):2:")"]']);
  args.checkParser(parser, ["((()))"],
    ['â”€<X>',
     '  â”œâ”€[(:1:"("]',
     '  â”œâ”€<X>',
     '  â”‚  â”œâ”€[(:2:"("]',
     '  â”‚  â”œâ”€<X>',
     '  â”‚  â”‚  â”œâ”€[(:3:"("]',
     '  â”‚  â”‚  â””â”€[):4:")"]',
     '  â”‚  â””â”€[):5:")"]',
     '  â””â”€[):6:")"]']);
}

void parser02(TestArgs args) {
  args.log('parser02');

  Tokenizer.Tokenizer tok = new Tokenizer.Tokenizer();
  tok.start("start");
  tok.join("start", "(")..addSet("(");
  tok.join("start", ")")..addSet(")");
  tok.setToken("(", "(");
  tok.setToken(")", ")");
  // 1. X â†’ ( X )
  // 2. X â†’ ğœ€
  Grammar.Grammar grammar = new Grammar.Grammar();
  grammar.start("X");
  grammar.newRule("X").addToken("(").addTerm("X").addToken(")");
  grammar.newRule("X");
  Parser.Parser parser = new Parser.Parser.fromGrammar(grammar, tok);
  args.checkParser(parser, [""],
    ['â”€<X>']);
  args.checkParser(parser, ["()"],
    ['â”€<X>',
     '  â”œâ”€[(:1:"("]',
     '  â”œâ”€<X>',
     '  â””â”€[):2:")"]']);
  args.checkParser(parser, ["((()))"],
    ['â”€<X>',
     '  â”œâ”€[(:1:"("]',
     '  â”œâ”€<X>',
     '  â”‚  â”œâ”€[(:2:"("]',
     '  â”‚  â”œâ”€<X>',
     '  â”‚  â”‚  â”œâ”€[(:3:"("]',
     '  â”‚  â”‚  â”œâ”€<X>',
     '  â”‚  â”‚  â””â”€[):4:")"]',
     '  â”‚  â””â”€[):5:")"]',
     '  â””â”€[):6:")"]']);
}

void parser03(TestArgs args) {
  args.log('parser03');

  Tokenizer.Tokenizer tok = new Tokenizer.Tokenizer();
  tok.start("start");
  tok.join("start", "a")..addSet("a");
  tok.join("start", "b")..addSet("b");
  tok.join("start", "d")..addSet("d");
  tok.setToken("a", "a");
  tok.setToken("b", "b");
  tok.setToken("d", "d");
  // 1. S â†’ b A d S
  // 2. S â†’ ğœ€
  // 3. A â†’ a A
  // 4. A â†’ ğœ€
  Grammar.Grammar grammar = new Grammar.Grammar();
  grammar.start("S");
  grammar.newRule("S").addToken("b").addTerm("A").addToken("d").addTerm("S");
  grammar.newRule("S");
  grammar.newRule("A").addToken("a").addTerm("A");
  grammar.newRule("A");
  Parser.Parser parser = new Parser.Parser.fromGrammar(grammar, tok);
  args.checkParser(parser, ["bd"],
    ['â”€<S>',
     '  â”œâ”€[b:1:"b"]',
     '  â”œâ”€<A>',
     '  â”œâ”€[d:2:"d"]',
     '  â””â”€<S>']);
  args.checkParser(parser, ["bad"],
    ['â”€<S>',
     '  â”œâ”€[b:1:"b"]',
     '  â”œâ”€<A>',
     '  â”‚  â”œâ”€[a:2:"a"]',
     '  â”‚  â””â”€<A>',
     '  â”œâ”€[d:3:"d"]',
     '  â””â”€<S>']);
  args.checkParser(parser, ["bdbadbd"],
    ['â”€<S>',
     '  â”œâ”€[b:1:"b"]',
     '  â”œâ”€<A>',
     '  â”œâ”€[d:2:"d"]',
     '  â””â”€<S>',
     '     â”œâ”€[b:3:"b"]',
     '     â”œâ”€<A>',
     '     â”‚  â”œâ”€[a:4:"a"]',
     '     â”‚  â””â”€<A>',
     '     â”œâ”€[d:5:"d"]',
     '     â””â”€<S>',
     '        â”œâ”€[b:6:"b"]',
     '        â”œâ”€<A>',
     '        â”œâ”€[d:7:"d"]',
     '        â””â”€<S>']);
}

void parser04(TestArgs args) {
  args.log('parser04');

  Tokenizer.Tokenizer tok = new Tokenizer.Tokenizer();
  tok.start("start");
  tok.join("start", "id")..addRange("a", "z");
  tok.join("id", "id")..addRange("a", "z");
  tok.join("start", "add")..addSet("+");
  tok.join("start", "mul")..addSet("*");
  tok.join("start", "open")..addSet("(");
  tok.join("start", "close")..addSet(")");
  tok.join("start", "start")..addSet(" ")..consume = true;
  tok.setToken("add", "+");
  tok.setToken("mul", "*");
  tok.setToken("open", "(");
  tok.setToken("close", ")");
  tok.setToken("id", "id");
  // 1. E â†’ E + E
  // 2. E â†’ E * E
  // 3. E â†’ ( E )
  // 4. E â†’ id
  Grammar.Grammar grammar = new Grammar.Grammar();
  grammar.start("E");
  grammar.newRule("E").addTerm("E").addToken("+").addTerm("E");
  grammar.newRule("E").addTerm("E").addToken("*").addTerm("E");
  grammar.newRule("E").addToken("(").addTerm("E").addToken(")");
  grammar.newRule("E").addToken("id");
  Parser.Parser parser = new Parser.Parser.fromGrammar(grammar, tok);
  
  // Test serializing and deserializing too.
  String data = parser.serialize().toString();
  parser = new Parser.Parser.deserialize(new Simple.Deserializer(data));
  args.checkParser(parser, ["a"],
    ['â”€<E>',
     '  â””â”€[id:1:"a"]']);
  args.checkParser(parser, ["(a + b)"],
    ['â”€<E>',
     '  â”œâ”€[(:1:"("]',
     '  â”œâ”€<E>',
     '  â”‚  â”œâ”€<E>',
     '  â”‚  â”‚  â””â”€[id:2:"a"]',
     '  â”‚  â”œâ”€[+:4:"+"]',
     '  â”‚  â””â”€<E>',
     '  â”‚     â””â”€[id:6:"b"]',
     '  â””â”€[):7:")"]']);
  args.checkParser(parser, ["a + b * c"],
    ['â”€<E>',
     '  â”œâ”€<E>',
     '  â”‚  â””â”€[id:1:"a"]',
     '  â”œâ”€[+:3:"+"]',
     '  â””â”€<E>',
     '     â”œâ”€<E>',
     '     â”‚  â””â”€[id:5:"b"]',
     '     â”œâ”€[*:7:"*"]',
     '     â””â”€<E>',
     '        â””â”€[id:9:"c"]']);
  args.checkParser(parser, ["a + (b * c) + d"],
    ['â”€<E>',
     '  â”œâ”€<E>',
     '  â”‚  â””â”€[id:1:"a"]',
     '  â”œâ”€[+:3:"+"]',
     '  â””â”€<E>',
     '     â”œâ”€<E>',
     '     â”‚  â”œâ”€[(:5:"("]',
     '     â”‚  â”œâ”€<E>',
     '     â”‚  â”‚  â”œâ”€<E>',
     '     â”‚  â”‚  â”‚  â””â”€[id:6:"b"]',
     '     â”‚  â”‚  â”œâ”€[*:8:"*"]',
     '     â”‚  â”‚  â””â”€<E>',
     '     â”‚  â”‚     â””â”€[id:10:"c"]',
     '     â”‚  â””â”€[):11:")"]',
     '     â”œâ”€[+:13:"+"]',
     '     â””â”€<E>',
     '        â””â”€[id:15:"d"]']);
}

void parser05(TestArgs args) {
  args.log('parser05');

  Tokenizer.Tokenizer tok = new Tokenizer.Tokenizer();
  tok.start("start");
  tok.join("start", "a").addSet("a");
  tok.setToken("a", "a");

  Grammar.Grammar grammar = new Grammar.Grammar();
  grammar.start("E");
  grammar.newRule("E");
  grammar.newRule("E").addTerm("E").addTerm("T");
  grammar.newRule("T").addToken("a");
  Parser.Parser parser = new Parser.Parser.fromGrammar(grammar, tok);
    
  args.checkParser(parser, ["aaa"],
    ['â”€<E>',
     '  â”œâ”€<E>',
     '  â”‚  â”œâ”€<E>',
     '  â”‚  â”‚  â”œâ”€<E>',
     '  â”‚  â”‚  â””â”€<T>',
     '  â”‚  â”‚     â””â”€[a:1:"a"]',
     '  â”‚  â””â”€<T>',
     '  â”‚     â””â”€[a:2:"a"]',
     '  â””â”€<T>',
     '     â””â”€[a:3:"a"]']);
}

void parser06(TestArgs args) {
  args.log('parser06');

  Tokenizer.Tokenizer tok = new Tokenizer.Tokenizer();
  tok.start("start");
  tok.joinToToken("start", "*").addSet("*");

  Grammar.Grammar grammar = new Grammar.Grammar();
  grammar.start("E");
  grammar.newRule("E");
  grammar.newRule("E").addTerm("T").addTerm("E");
  grammar.newRule("T").addToken("*");

  args.checkParserBuildError(grammar, tok,
    ['Exception: Errors while building parser:',
     'state 0:',
     '  <startTerm> â†’ â€¢ <E> [eofToken]',
     '  <E> â†’ â€¢',
     '  <E> â†’ â€¢ <T> <E>',
     '  <T> â†’ â€¢ [*]',
     '  <E>: goto state 1',
     '  <T>: goto state 2',
     '  [*]: goto state 3',
     'state 1:',
     '  <startTerm> â†’ <E> â€¢ [eofToken]',
     'state 2:',
     '  <E> â†’ <T> â€¢ <E>',
     '  <E> â†’ â€¢',
     '  <E> â†’ â€¢ <T> <E>',
     '  <T> â†’ â€¢ [*]',
     '  <E>: goto state 4',
     '  <T>: goto state 2',
     '  [*]: goto state 3',
     'state 3:',
     '  <T> â†’ [*] â€¢',
     'state 4:',
     '  <E> â†’ <T> <E> â€¢',
     '',
     'Infinite goto loop found in term T between the state(s) [2].']);
}
